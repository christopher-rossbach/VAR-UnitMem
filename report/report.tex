
\documentclass{article} % For LaTeX2e
\usepackage{iclr2025_conference,times}

% Optional math commands from https://github.com/goodfeli/dlbook_notation.
\input{math_commands.tex}

\usepackage{hyperref}
\usepackage{url}


\title{Searching for Memorization\\in Visual Autoregressive Models}


\author{Christopher Roßbach\\
Friedrich-Alexander-Universität Erlangen-Nürnberg \\
Erlangen, 91058, Germany \\
\texttt{christopher.rossbach@fau.de}}

\newcommand{\fix}{\marginpar{FIX}}
\newcommand{\new}{\marginpar{NEW}}

%\iclrfinalcopy % Uncomment for camera-ready version, but NOT for submission.
\begin{document}


\maketitle

\begin{abstract}
This work investigates memorization phenomena in visual autoregressive models (VAR), focusing on identifying and quantifying memorization at the unit (neuron/channel) level.
By adapting the UnitMem methodology, we analyze where memorization occurs within autoregressive models for images.
More specifically, we analyse at which depth and in which layer types of the self attention blocks memorization occurs and which samples are most prone to be memorized.
\end{abstract}
\section{Methodology}
We apply the UnitMem methodology \citep{wangLocalizingMemorizationSSL2024} to visual autoregressive models (VAR) \citep{tianVisualAutoregressiveModeling2024}.
For this, we made a small modification to the original UnitMem formulation to adapt it to VAR models.
The original UnitMem defines the mean activation of a unit $u$ on a sample $x$ as:
\begin{equation}
    \mu_u(x) = \mathbb{E}_{x^\prime\sim\text{Aug}(x)}[\text{activation}_u(x^\prime)],
\end{equation}
where $\text{Aug}(x)$ is a probabilistic augmentation of $x$.
We take the absolute value of the activation to account for the possible negative activations in VAR models and capture the excitement of a unit regardless of the sign and define the mean activation as:
\begin{equation}
   \label{eq:activation_mean}
    \mu_u(x) = \mathbb{E}_{x^\prime\sim\text{Aug}(x)}[|\text{activation}_u(x^\prime)|].
\end{equation}
Further we use the VAR model in the same way as during training.
That means, we that we
\begin{enumerate}
   \item supply the model with $([s], e_1, e_2, ..., e_{n-1})$ where $[s]$ is the start token obtained from the label and $e_1, e_2, ..., e_{n}$ are the upscaled image token embeddings of the image $x$ obtained from the multi scale token maps $(r_1, r_2, ..., r_{n-1})$ and 
   \item run with teacher-forced inputs.
\end{enumerate}

\section{Experiments}
The Experiments are performed on the 16 blocks deep pretrained VAR model provided by \citet{tianVisualAutoregressiveModeling2024}\footnote{https://github.com/FoundationVision/VAR} which is trained on ImageNet \citep{dengImageNetLargeScaleHierarchical2009a} with 256x256 resolution.
We obtain our dataset $\mathcal{D}^\prime$ by sampling 256 images from 32 different classes\footnote{Experiments on the influence of the class on memorization may motivate a different split} of the ImageNet-1k training set.
To approximate the expectation in equation \ref{eq:activation_mean}, we use 8 augmentations per image.\footnote{The only randomized augmentation used is random cropping from a 1.25x resized image, so 8 augmentations may be sufficient.}
We restricted our search to the \texttt{attn.proj}, \texttt{ffn.fc1}, and \texttt{ffn.fc2} layers of each self-attention block, totaling to $16\cdot (1024 + 4096 + 1024) = 98,304$ units.
For each of the 8192 images in $\mathcal{D}^\prime$ we calculated the mean activation for each unit in the selected layers and and stored the result.
From that data we calculated the $\texttt{UnitMem}_{\mathcal{D}^\prime}(u)$ for each unit $u$.
\subsection{Localization}
We define the set of the units with the highest 10\% UnitMem values $U_{10}$ as:
\begin{equation}\label{eq:mem_units}
   \mathcal{U}_{10} = \{u \in \mathcal{U} \mid\texttt{UnitMem}_{\mathcal{D}^\prime}(u) \geq P_{90}(\{\texttt{UnitMem}_{\mathcal{D}^\prime}(u) \mid u \in \mathcal{U}\})\},
\end{equation}
where $P_{90}$ is the 90th percentile function and $\mathcal{U}$ is the set of all units.
For each $x \in \mathcal{D}^\prime$ we define the number of highly memorizing units maximally activated by $x$ as:
\[\text{NumMemUnits}(x) = |\{u \in \mathcal{U}_{10} \mid \mu_u(x) = \max(\{\mu_u(x^\prime) \mid x^\prime \in \mathcal{D}^\prime\})\}|.\]

We find that highly memorizing units are distributed over all blocks, as shown in Figure \ref{fig:mem_units_per_layer_num}.
We notice a strong decrease with increasing depth, except the last block, where momorization peaks.
Also a wave pattern with local maxima at the block numbers 0, 5, 10 and 15 is visible.
A very similar pattern can be observed when looking at the average \texttt{UnitMem} values over all units instead of the the distribution of $\mathcal{U}_{10}$ (see Figure~\ref{fig:mem_avg_per_layer_num}).

We also looked at the position \textit{within} a block, i.e. the layer type (\texttt{attn.proj}, \texttt{ffn.fc1} and \texttt{ffn.fc2}), and noted that the majority of higly memorizing units are located in the \texttt{ffn.fc1} layer, as shown in Figure \ref{fig:mem_units_per_layer_type}.
This trend even holds true when adjusting for the fact that most of the units are located in the \texttt{ffn.fc1} layer (4096 out of 6144 units per block).
When looking at the average \texttt{UnitMem} values per layer type (see Figure~\ref{fig:mem_avg_per_layer_type}), we see a more even distribution, indicating that \texttt{ffn.fc1} layers also contain a lot of low memorizing units.
Figure~\ref{fig:mem_max_per_layer_num} shows that the unit with maximal \texttt{UnitMem} value of each block is always found in the \texttt{ffn.fc1} layer.

\subsection{Memorized Samples}
Our experiments show that only a relatively small number of samples are responsible for the high \texttt{UnitMem} values of the Units in $\mathcal{U}_{10}$.
Only about 27.3\% of the samples in $\mathcal{D}^\prime$ are responsible for the maximal activation of at least one unit in $\mathcal{U}_{10}$ (i.e. have a \texttt{NumMemUnits} value greater than 0).
At the same time, already 1\% of the samples in $\mathcal{D}^\prime$ are responsible for the maximal activation of 49\% of the units in $\mathcal{U}_{10}$.
10\% of the samples in $\mathcal{D}^\prime$ are responsible for the maximal activation of 83\% of the units in $\mathcal{U}_{10}$.
\footnote{TODO: insert numbers from last experiments}
\footnote{Additionally, not documented experiments suggest that the portion of memorized samples may be even smaller when looking at a larger dataset $\mathcal{D}^\prime$.}

We also looked at the samples that maximally activate the highest number of highly memorizing units, i.e. the samples with the highest \texttt{NumMemUnits} value, and performed a qualitative assesment.
We find that a huge portion of these samples show a spacially repeating high frequency pattern or huge areas of constant color (see Figure \ref{fig:mem_samples}).

We also (qualitatively) looked how the memorized samples differ depending on the block depth.
For that we looked at the samples that are maximally activating the top 10 units (in terms of \texttt{UnitMem} value) in the \texttt{ffn.fc1} layer of blocks 0, 5, 10 and 15.
While the samples for the blocks 0 and 5 are quite diverse, in block 10 7 out of 10 samples stem from the same class and in block 15 all top 10 \texttt{UnitMem} scores originate from the same image as can be seen in Figure~\ref{fig:mem_samples_per_layer}.
\subsubsection{A Look at the Validation Set}\label{look_at_val}\footnote{This section is not yet fully verified.}
For sanity check, we construct a dataset $\mathcal{D}_{val}$ of 256 images per class for the same 32 classes as above from the ImageNet-1k validation set.
We use the data set $\mathcal{D}_{all} = \mathcal{D}_{val} \cup \mathcal{D}^\prime$ consisting of 16,384 images from the validation set and the images from the training set.

We expect to see less memorization, for samples in $\mathcal{D}_{val}$ as the model was not trained on these images.\footnote{maybe it was?}
Figure~\ref{fig:val_respon_num} shows that the number of \texttt{UnitMem} scores caused by samples from the validation set is slightly higher than for samples from the training set.
Restricting the analysis to only the units in $\mathcal{U}_{10,\mathcal{D}_{all}}$ (obtained as in equation~\ref{eq:mem_units} but on $\mathcal{D}_{all}$) we see that this trend increases.

\section{Future Work}
\begin{enumerate}
   \item The last observation (less diversity in memorized samples with increasing depth) is very interesting and should be further investigated in a quantitative manner.
   The per-classes as well as the per-image distribution are interesting aspects to look at.
   \item The same experiments should be performed on different model depths
   \item The influence of the class on memorization may be interesting to explore.
   \item The findings in~\ref{look_at_val} are unexpected and should be further investigated (they may not even be correct).
\end{enumerate}



\bibliography{iclr2025_conference}
\bibliographystyle{iclr2025_conference}

\appendix
\section{Appendix}
You may include other additional sections here.


\end{document}
